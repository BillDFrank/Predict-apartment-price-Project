{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from fake_useragent import UserAgent\n",
    "from ConnectDB import connect_to_database, save_data_to_db, update_details_in_db, get_last_scraped_page_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(page):\n",
    "    \"\"\"\n",
    "    Scrape a single page of listings from Imovirtual (Portuguese version).\n",
    "\n",
    "    Args:\n",
    "        page (int): The page number to scrape.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the basic data.\n",
    "    \"\"\"\n",
    "    url = f\"https://www.imovirtual.com/pt/resultados/comprar/apartamento/todo-o-pais?viewType=listing&page={page}\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"‚ùå Failed to retrieve page {page}. Status code: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    listings = soup.find_all(\"article\", {\"data-cy\": \"listing-item\"})\n",
    "    \n",
    "    if not listings:\n",
    "        print(f\"‚ö†Ô∏è No listings found on page {page}. Check selectors or page structure.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for listing in listings:\n",
    "        # Extract URL\n",
    "        a_tag = listing.find(\"a\")\n",
    "        ad_url = \"https://www.imovirtual.com\" + a_tag.get(\"href\") if a_tag and a_tag.get(\"href\") else np.nan\n",
    "\n",
    "        # Extract title\n",
    "        title_el = listing.find(\"p\", class_=\"css-u3orbr\")\n",
    "        title = title_el.get_text(strip=True) if title_el else np.nan\n",
    "\n",
    "        # Extract price\n",
    "        price_el = listing.find(\"span\", class_=\"css-2bt9f1\")\n",
    "        price = price_el.get_text(strip=True) if price_el else np.nan\n",
    "\n",
    "        # Extract location\n",
    "        location_el = listing.find(\"p\", class_=\"css-42r2ms\")\n",
    "        location = location_el.get_text(strip=True) if location_el else np.nan\n",
    "\n",
    "        # Extract details (rooms and area)\n",
    "        dl = listing.find(\"dl\", class_=\"css-12dsp7a\")\n",
    "        rooms, area = np.nan, np.nan  # Default to NaN (better for numerical processing)\n",
    "        if dl:\n",
    "            dt_dd = {dt.text.strip(): dd.text.strip() for dt, dd in zip(dl.find_all(\"dt\"), dl.find_all(\"dd\"))}\n",
    "            \n",
    "            # Rooms (Tipologia)\n",
    "            rooms = dt_dd.get(\"Tipologia\", np.nan)\n",
    "            \n",
    "            # Area can be under \"√Årea\" or \"Zona\"\n",
    "            area = dt_dd.get(\"√Årea\", dt_dd.get(\"Zona\", np.nan))\n",
    "            if isinstance(area, str):\n",
    "                area = area.split()[0]  # Extract numeric value from \"120 m¬≤\"\n",
    "\n",
    "        scrape_date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "        data.append([page, title, price, location, rooms, area, ad_url, scrape_date])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"Page\", \"Title\", \"Price\", \"Location\", \"Rooms\", \"Area\", \"URL\", \"ScrapeDate\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_listings(num_pages=1, force_start_page=None):\n",
    "    \"\"\"\n",
    "    Scrape multiple pages of basic advertising info and save each page to the database.\n",
    "\n",
    "    Args:\n",
    "        num_pages (int): Number of pages to scrape.\n",
    "        force_start_page (int, optional): If provided, forces the scraper to start at this page number,\n",
    "                                          overriding the resume logic.\n",
    "        upper_limit (int): The maximum page number to scrape (if the starting page exceeds this,\n",
    "                           the scraper stops).\n",
    "    \"\"\"\n",
    "    conn = connect_to_database()\n",
    "    if conn is None:\n",
    "        print(\"‚ùå Database connection failed. Exiting.\")\n",
    "        return\n",
    "\n",
    "    today_str = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Determine the starting page\n",
    "    if force_start_page is not None:\n",
    "        start_page = force_start_page\n",
    "    else:\n",
    "        last_page_today = get_last_scraped_page_today(conn, today_str)\n",
    "        # If there are records for today, resume from last_page_today + 1; otherwise, start from page 1.\n",
    "        start_page = last_page_today + 1 if last_page_today > 0 else 1\n",
    "\n",
    "    if start_page > num_pages:\n",
    "        print(f\"‚ÑπÔ∏è Already scraped up to page {start_page - 1} for today (num pages {num_pages}). No new pages to scrape.\")\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    end_page = num_pages\n",
    "    print(f\"üü¢ Scraping pages {start_page} to {end_page}...\")\n",
    "\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        df = scrape_page(page)\n",
    "        if not df.empty:\n",
    "            save_data_to_db(conn, df)\n",
    "            print(f\"‚úÖ Page {page} data saved to database.\")\n",
    "        else:\n",
    "            print(f\"‚ÑπÔ∏è No data found on page {page}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        time.sleep(random.uniform(1, 3))  # Delay between page requests\n",
    "\n",
    "    conn.close()\n",
    "    print(\"üéØ Basic scraping completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_details():\n",
    "    \"\"\"\n",
    "    For each advertising record missing bathrooms, construction_year, or energetic_certificate information,\n",
    "    scrape the advertising URL to extract these details and update the database.\n",
    "    \"\"\"\n",
    "    ua = UserAgent()\n",
    "    session = requests.Session()\n",
    "    \n",
    "    conn = connect_to_database()\n",
    "    if conn is None:\n",
    "        print(\"‚ùå Database connection failed. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Select records that need detail extraction (where any of the detail columns are NULL)\n",
    "    query = \"SELECT id, url FROM advertisings WHERE bathrooms IS NULL OR construction_year IS NULL OR energetic_certificate IS NULL\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    records = cursor.fetchall()\n",
    "    \n",
    "    print(f\"üü¢ Found {len(records)} records needing detail extraction.\")\n",
    "    \n",
    "    for rec in records:\n",
    "        record_id, ad_url = rec[0], rec[1]\n",
    "        try:\n",
    "            response = session.get(ad_url, headers={'User-Agent': ua.random}, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"‚ö†Ô∏è Failed to retrieve details for record {record_id}. Status code: {response.status_code}\")\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # --- Extract Bathrooms ---\n",
    "            try:\n",
    "                buttons = soup.find_all('button', class_='eezlw8k1 css-ds0a69')\n",
    "                bathrooms = buttons[2].find('div', class_='css-1ftqasz').get_text(strip=True) if len(buttons) >= 3 else \"N/A\"\n",
    "            except Exception:\n",
    "                bathrooms = \"N/A\"\n",
    "            \n",
    "            # --- Extract Construction Year ---\n",
    "            try:\n",
    "                construction_year = \"N/A\"\n",
    "                divs = soup.find_all('div', class_='css-t7cajz e16p81cp1')\n",
    "                for div in divs:\n",
    "                    p_elements = div.find_all('p', class_='e16p81cp2 css-nlohq6')\n",
    "                    for i, p in enumerate(p_elements):\n",
    "                        if \"Ano de constru√ß√£o\" in p.get_text(strip=True):\n",
    "                            construction_year = p_elements[i+1].get_text(strip=True) if i+1 < len(p_elements) else \"N/A\"\n",
    "                            break\n",
    "                    if construction_year != \"N/A\":\n",
    "                        break\n",
    "            except Exception:\n",
    "                construction_year = \"N/A\"\n",
    "            \n",
    "            # --- Extract Energetic Certificate ---\n",
    "            try:\n",
    "                energetic_certificate = \"N/A\"\n",
    "                for div in divs:\n",
    "                    p_elements = div.find_all('p', class_='e16p81cp2 css-nlohq6')\n",
    "                    for i, p in enumerate(p_elements):\n",
    "                        if \"Certificado energ√©tico\" in p.get_text(strip=True):\n",
    "                            energetic_certificate = p_elements[i+1].get_text(strip=True) if i+1 < len(p_elements) else \"N/A\"\n",
    "                            break\n",
    "                    if energetic_certificate != \"N/A\":\n",
    "                        break\n",
    "            except Exception:\n",
    "                energetic_certificate = \"N/A\"\n",
    "            \n",
    "            update_details_in_db(conn, record_id, bathrooms, construction_year, energetic_certificate)\n",
    "            time.sleep(random.uniform(0.5, 1.5))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing record {record_id} ({ad_url}): {e}\")\n",
    "    \n",
    "    conn.close()\n",
    "    print(\"üéØ Detail extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_listings()\n",
    "scrape_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imovirtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
