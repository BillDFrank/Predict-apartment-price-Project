{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = \"Files\"\n",
    "LISTINGS_CSV = os.path.join(BASE_DIR, \"apartmentsPortugal.csv\")\n",
    "CONSOLIDATED_CSV = os.path.join(BASE_DIR, \"Consolidated.csv\")\n",
    "BASE_URL = \"https://www.imovirtual.com/pt/resultados/comprar/apartamento/todo-o-pais?page=\"\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Ensure directory exists\n",
    "os.makedirs(BASE_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_page(page_num):\n",
    "    \"\"\"Scrape data from a single listing page with columns matching consolidated.csv\"\"\"\n",
    "    url = f\"{BASE_URL}{page_num}\"\n",
    "    print(f\"Fetching URL: {url}\")  # Debugging line to verify the requested page\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve page {page_num}. Status code: {response.status_code}\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    articles = soup.find_all('article', {'data-cy': 'listing-item'})\n",
    "    \n",
    "    if not articles:\n",
    "        return None\n",
    "    \n",
    "    # Initialize all columns present in consolidated.csv\n",
    "    data = {\n",
    "        'URL': [],\n",
    "        'Titles': [],\n",
    "        'Price': [],\n",
    "        'Location': [],\n",
    "        'Rooms': [],\n",
    "        'Areas': [],\n",
    "        'Bathroom': [],\n",
    "        'Listing Type': [],\n",
    "        'Useful area': [],\n",
    "        'Gross area': [],\n",
    "        'Construction year': [],\n",
    "        'Energetic certificate': [],\n",
    "        'DateScraped': [],\n",
    "        'Page': [],\n",
    "        'Description': []\n",
    "    }\n",
    "    \n",
    "    scrape_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    for article in articles:\n",
    "        # Existing fields\n",
    "        data['URL'].append(\n",
    "            f\"https://www.imovirtual.com{article.find('a')['href']}\" \n",
    "            if article.find('a') else np.nan\n",
    "        )\n",
    "        data['Titles'].append(\n",
    "            article.find('p', class_='css-u3orbr').text.strip() \n",
    "            if article.find('p', class_='css-u3orbr') else np.nan\n",
    "        )\n",
    "        data['Price'].append(\n",
    "            article.find('span', class_='css-2bt9f1').text.strip() \n",
    "            if article.find('span', class_='css-2bt9f1') else np.nan\n",
    "        )\n",
    "        data['Location'].append(\n",
    "            article.find('p', class_='css-42r2ms').text.strip() \n",
    "            if article.find('p', class_='css-42r2ms') else np.nan\n",
    "        )\n",
    "        \n",
    "        # Rooms and Areas\n",
    "        dl = article.find('dl', class_='css-12dsp7a')\n",
    "        if dl:\n",
    "            dt_dd = {dt.text.strip(): dd.text.strip() \n",
    "                    for dt, dd in zip(dl.find_all('dt'), dl.find_all('dd'))}\n",
    "            data['Rooms'].append(dt_dd.get('Tipologia', np.nan))\n",
    "            area = dt_dd.get('Zona', np.nan)\n",
    "            data['Areas'].append(area.split()[0] if area else np.nan)\n",
    "        else:\n",
    "            data['Rooms'].append(np.nan)\n",
    "            data['Areas'].append(np.nan)\n",
    "        \n",
    "        # Empty columns to be filled in Part 2\n",
    "        data['Bathroom'].append(np.nan)\n",
    "        data['Listing Type'].append(np.nan)\n",
    "        data['Useful area'].append(np.nan)\n",
    "        data['Gross area'].append(np.nan)\n",
    "        data['Construction year'].append(np.nan)\n",
    "        data['Energetic certificate'].append(np.nan)\n",
    "        data['Description'].append(np.nan)\n",
    "        data['DateScraped'].append(scrape_date)\n",
    "        data['Page'].append(page_num)\n",
    "    \n",
    "    # Validate lengths\n",
    "    lengths = [len(v) for v in data.values()]\n",
    "    if len(set(lengths)) != 1:\n",
    "        print(f\"Data length mismatch in page {page_num}: {lengths}\")\n",
    "        return None\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def scrape_listings(num_pages):\n",
    "    \"\"\"Scrape multiple listing pages with resume support.\"\"\"\n",
    "    if os.path.exists(LISTINGS_CSV):\n",
    "        df_existing = pd.read_csv(LISTINGS_CSV)\n",
    "        start_page = df_existing['Page'].max() + 1 if 'Page' in df_existing.columns else 1\n",
    "    else:\n",
    "        start_page = 1\n",
    "    \n",
    "    end_page = start_page + num_pages - 1\n",
    "    session = requests.Session()\n",
    "    \n",
    "    for page in range(start_page, end_page + 1):\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        df_page = scrape_page(page)\n",
    "        if df_page is None:\n",
    "            print(f\"No data on page {page}. Stopping.\")\n",
    "            break\n",
    "        \n",
    "        # Save incrementally\n",
    "        df_page.to_csv(LISTINGS_CSV, mode='a', header=not os.path.exists(LISTINGS_CSV), index=False)\n",
    "        print(f\"Page {page} saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_details():\n",
    "    \"\"\"Scrape detailed information with corrected column names\"\"\"\n",
    "    df = pd.read_csv(LISTINGS_CSV)\n",
    "    \n",
    "    # Identify URLs needing processing\n",
    "    mask = df['Construction year'].isna() | df['Bathroom'].isna()\n",
    "    indices = df[mask].index.tolist()\n",
    "    \n",
    "    ua = UserAgent()\n",
    "    session = requests.Session()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        try:\n",
    "            url = df.loc[idx, 'URL']\n",
    "            response = session.get(url, headers={'User-Agent': ua.random}, timeout=10)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "                # Get useful area (default to \"N/A\" if not available)\n",
    "                df.at[idx, 'Useful area'] = \"N/A\"\n",
    "\n",
    "                # Get gross area, rooms, and bathrooms\n",
    "                try:\n",
    "                    buttons = soup.find_all('button', class_='eezlw8k1 css-ds0a69')\n",
    "                    if len(buttons) >= 3:\n",
    "                        area_div = buttons[0].find('div', class_='css-1ftqasz')\n",
    "                        df.at[idx, 'Gross area'] = area_div.get_text(strip=True) if area_div else \"N/A\"\n",
    "\n",
    "                        rooms_div = buttons[1].find('div', class_='css-1ftqasz')\n",
    "                        df.at[idx, 'Rooms'] = rooms_div.get_text(strip=True) if rooms_div else \"N/A\"\n",
    "\n",
    "                        bathrooms_div = buttons[2].find('div', class_='css-1ftqasz')\n",
    "                        df.at[idx, 'Bathroom'] = bathrooms_div.get_text(strip=True) if bathrooms_div else \"N/A\"\n",
    "                    else:\n",
    "                        df.at[idx, 'Gross area'] = \"N/A\"\n",
    "                        df.at[idx, 'Rooms'] = \"N/A\"\n",
    "                        df.at[idx, 'Bathroom'] = \"N/A\"\n",
    "                except AttributeError:\n",
    "                    df.at[idx, 'Gross area'] = \"N/A\"\n",
    "                    df.at[idx, 'Rooms'] = \"N/A\"\n",
    "                    df.at[idx, 'Bathroom'] = \"N/A\"\n",
    "\n",
    "                # Get construction year\n",
    "                try:\n",
    "                    divs = soup.find_all('div', class_='css-t7cajz e15n0fyo1')\n",
    "                    construction_year = \"N/A\"\n",
    "                    for div in divs:\n",
    "                        p_elements = div.find_all('p', class_='e15n0fyo2 css-nlohq6')\n",
    "                        for i, p in enumerate(p_elements):\n",
    "                            if \"Ano de construção\" in p.get_text(strip=True):\n",
    "                                if i + 1 < len(p_elements):\n",
    "                                    construction_year = p_elements[i + 1].get_text(strip=True)\n",
    "                                break\n",
    "                        if construction_year != \"N/A\":\n",
    "                            break\n",
    "                    df.at[idx, 'Construction year'] = construction_year\n",
    "                except (AttributeError, IndexError):\n",
    "                    df.at[idx, 'Construction year'] = \"N/A\"\n",
    "\n",
    "                # Get energetic certificate\n",
    "                try:\n",
    "                    energetic_certificate = \"N/A\"\n",
    "                    for div in divs:\n",
    "                        p_elements = div.find_all('p', class_='e15n0fyo2 css-nlohq6')\n",
    "                        for i, p in enumerate(p_elements):\n",
    "                            if \"Certificado energético\" in p.get_text(strip=True):\n",
    "                                if i + 1 < len(p_elements):\n",
    "                                    energetic_certificate = p_elements[i + 1].get_text(strip=True)\n",
    "                                break\n",
    "                        if energetic_certificate != \"N/A\":\n",
    "                            break\n",
    "                    df.at[idx, 'Energetic certificate'] = energetic_certificate\n",
    "                except (AttributeError, IndexError):\n",
    "                    df.at[idx, 'Energetic certificate'] = \"N/A\"\n",
    "\n",
    "                # Save progress every 50 URLs\n",
    "                if (i + 1) % 50 == 0:\n",
    "                    df.to_csv(LISTINGS_CSV, index=False)\n",
    "\n",
    "                time.sleep(random.uniform(0.5, 1.5))\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping {url} due to status code {response.status_code}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "    \n",
    "    df.to_csv(LISTINGS_CSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data():\n",
    "    \"\"\"Merge data while preserving existing structure\"\"\"\n",
    "    if os.path.exists(CONSOLIDATED_CSV):\n",
    "        consolidated = pd.read_csv(CONSOLIDATED_CSV)\n",
    "    else:\n",
    "        consolidated = pd.DataFrame(columns=[\n",
    "            'URL', 'Titles', 'Price', 'Location', 'Rooms', 'Areas',\n",
    "            'Bathroom', 'Listing Type', 'Useful area', 'Gross area',\n",
    "            'Construction year', 'Energetic certificate',\n",
    "            'DateScraped', 'Page', 'Description'\n",
    "        ])\n",
    "    \n",
    "    new_data = pd.read_csv(LISTINGS_CSV)\n",
    "    \n",
    "    # Convert numeric columns\n",
    "    numeric_cols = ['Construction year', 'Page']\n",
    "    for col in numeric_cols:\n",
    "        new_data[col] = pd.to_numeric(new_data[col], errors='coerce')\n",
    "    \n",
    "    # Merge and deduplicate\n",
    "    combined = pd.concat([consolidated, new_data], ignore_index=True)\n",
    "    combined = combined.drop_duplicates('URL', keep='last')\n",
    "    \n",
    "    # Remove temporary columns\n",
    "    combined = combined.loc[:, ~combined.columns.str.contains('Unnamed')]\n",
    "    \n",
    "    combined.to_csv(CONSOLIDATED_CSV, index=False)\n",
    "    print(f\"Merged data saved. Total records: {len(combined)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_listings(2)\n",
    "scrape_details()\n",
    "merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings = pd.read_csv(LISTINGS_CSV, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
