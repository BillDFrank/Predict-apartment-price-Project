{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1693149968655,
     "user": {
      "displayName": "William Dieter Frank",
      "userId": "11059887809384858726"
     },
     "user_tz": -60
    },
    "id": "LlvU3lsoE2ai"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90000 entries, 0 to 89999\n",
      "Data columns (total 13 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   URL                    90000 non-null  object \n",
      " 1   Titles                 90000 non-null  object \n",
      " 2   Price                  90000 non-null  object \n",
      " 3   Location               90000 non-null  object \n",
      " 4   Rooms                  90000 non-null  object \n",
      " 5   Areas                  90000 non-null  object \n",
      " 6   Bathrooms              81941 non-null  object \n",
      " 7   Listing Type           62115 non-null  object \n",
      " 8   Useful area            89747 non-null  object \n",
      " 9   Gross area             71697 non-null  object \n",
      " 10  Construction year      49716 non-null  float64\n",
      " 11  Energetic certificate  89575 non-null  object \n",
      " 12  Enterprise             80494 non-null  object \n",
      "dtypes: float64(1), object(12)\n",
      "memory usage: 8.9+ MB\n"
     ]
    }
   ],
   "source": [
    "consolidated_df = pd.read_csv(\"Files/Consolidated.csv\", index_col=False)\n",
    "consolidated_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1693149968656,
     "user": {
      "displayName": "William Dieter Frank",
      "userId": "11059887809384858726"
     },
     "user_tz": -60
    },
    "id": "WYdtP9alKHZ0"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://www.imovirtual.com/comprar/apartamento/?page=\"\n",
    "num_pages = 1  # Specify the number of pages to scrape\n",
    "df = []\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1693149968910,
     "user": {
      "displayName": "William Dieter Frank",
      "userId": "11059887809384858726"
     },
     "user_tz": -60
    },
    "id": "5lbVXoHaNKO7"
   },
   "outputs": [],
   "source": [
    "def scrape_data(url):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve data. HTTP Status code: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    article_elements = soup.find_all('article', {'data-cy': 'listing-item'})\n",
    "\n",
    "    # Lists to store extracted data\n",
    "    titles = []\n",
    "    prices = []\n",
    "    locations = []\n",
    "    rooms = []\n",
    "    areas = []\n",
    "    urls = []\n",
    "    dates = []\n",
    "\n",
    "    scrape_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "    for article_element in article_elements:\n",
    "        # Extract title\n",
    "        title_element = article_element.find('p', class_='css-u3orbr e1g5xnx10')\n",
    "        titles.append(title_element.text.strip() if title_element else np.nan)\n",
    "\n",
    "        # Extract URL\n",
    "        url_element = article_element.find('a')\n",
    "        urls.append(f\"https://www.imovirtual.com{url_element['href']}\" if url_element else np.nan)\n",
    "\n",
    "        # Extract price\n",
    "        price_element = article_element.find('span', class_='css-2bt9f1 evk7nst0')\n",
    "        prices.append(price_element.text.strip() if price_element else np.nan)\n",
    "\n",
    "        # Extract location\n",
    "        location_element = article_element.find('p', class_='css-42r2ms eejmx80')\n",
    "        locations.append(location_element.text.strip() if location_element else np.nan)\n",
    "\n",
    "        # Extract room count and area from <dl>\n",
    "        dl_element = article_element.find('dl', class_='css-12dsp7a e1clni9t1')\n",
    "        room = np.nan\n",
    "        area = np.nan\n",
    "\n",
    "        if dl_element:\n",
    "            dt_elements = dl_element.find_all('dt')\n",
    "            dd_elements = dl_element.find_all('dd')\n",
    "\n",
    "            # Ensure there is a match between <dt> and <dd>\n",
    "            for dt, dd in zip(dt_elements, dd_elements):\n",
    "                if dt.text.strip() == \"Tipologia\":\n",
    "                    room = dd.text.strip()\n",
    "                if dt.text.strip() == \"Zona\":\n",
    "                    area = dd.text.strip().split(\" \")[0]  # Extract just the numeric value\n",
    "                    break\n",
    "\n",
    "        rooms.append(room)\n",
    "        areas.append(area)\n",
    "\n",
    "        # Add scraping date for each item\n",
    "        dates.append(scrape_date)\n",
    "    \n",
    "    print(f\"Titles: {len(titles)}, Prices: {len(prices)}, Locations: {len(locations)}, Rooms: {len(rooms)}, Areas: {len(areas)}, URLs: {len(urls)}\")\n",
    "\n",
    "    return {\n",
    "        'titles': titles,\n",
    "        'prices': prices,\n",
    "        'locations': locations,\n",
    "        'rooms': rooms,\n",
    "        'areas': areas,\n",
    "        'urls': urls,\n",
    "        'dates': dates\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3881,
     "status": "ok",
     "timestamp": 1693150008761,
     "user": {
      "displayName": "William Dieter Frank",
      "userId": "11059887809384858726"
     },
     "user_tz": -60
    },
    "id": "aLZZyI47mJ8U",
    "outputId": "20b8c2a0-589c-4a5c-92b1-b4da6f0015a7"
   },
   "outputs": [],
   "source": [
    "titles = []\n",
    "prices = []\n",
    "locations = []\n",
    "rooms = []\n",
    "areas = []\n",
    "urls = []\n",
    "dates = []\n",
    "\n",
    "for page in range(1, num_pages + 1):\n",
    "    print(f\"Processing page {page}...\")\n",
    "    page_url = base_url + str(page)\n",
    "    \n",
    "    # Scrape data for the current page\n",
    "    scraped_data = scrape_data(page_url)\n",
    "    if not scraped_data:\n",
    "        print(f\"Skipping page {page} due to scraping error.\")\n",
    "        continue\n",
    "\n",
    "    page_titles = scraped_data[\"titles\"]\n",
    "    page_prices = scraped_data[\"prices\"]\n",
    "    page_locations = scraped_data[\"locations\"]\n",
    "    page_rooms = scraped_data[\"rooms\"]\n",
    "    page_areas = scraped_data[\"areas\"]\n",
    "    page_urls = scraped_data[\"urls\"]\n",
    "    page_dates = scraped_data[\"dates\"]\n",
    "\n",
    "    # Validate that all lists have the same length\n",
    "    lengths = [\n",
    "        len(page_titles),\n",
    "        len(page_prices),\n",
    "        len(page_locations),\n",
    "        len(page_rooms),\n",
    "        len(page_areas),\n",
    "        len(page_urls),\n",
    "        len(page_dates)\n",
    "    ]\n",
    "    \n",
    "    if len(set(lengths)) != 1:\n",
    "        print(f\"Warning: Data length mismatch on page {page}. Skipping...\")\n",
    "        print(f\"Lengths: {lengths}\")\n",
    "        continue\n",
    "\n",
    "    # Append to the main lists\n",
    "    titles += page_titles\n",
    "    prices += page_prices\n",
    "    locations += page_locations\n",
    "    rooms += page_rooms\n",
    "    areas += page_areas\n",
    "    urls += page_urls\n",
    "    dates += page_dates\n",
    "\n",
    "    print(f\"Page {page} processed: {lengths[0]} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1810
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1693150012374,
     "user": {
      "displayName": "William Dieter Frank",
      "userId": "11059887809384858726"
     },
     "user_tz": -60
    },
    "id": "XbfHzim7Nouh",
    "outputId": "1fa77515-f61f-4d95-9736-94b8145f0a01"
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Title\": titles,\n",
    "    \"Price\": prices,\n",
    "    \"Location\": locations,\n",
    "    \"Rooms\": rooms,\n",
    "    \"Area\": areas,\n",
    "    \"URL\": urls,\n",
    "    \"DateScraped\": dates\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 279,
     "status": "ok",
     "timestamp": 1693152392452,
     "user": {
      "displayName": "William Dieter Frank",
      "userId": "11059887809384858726"
     },
     "user_tz": -60
    },
    "id": "XcBQbllrJvqp"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"Files/apartmentsPortugal.csv\", index=\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Load your DataFrame containing property URLs\n",
    "df = pd.read_csv('Files/apartmentsPortugal.csv')\n",
    "\n",
    "# Create empty columns in the existing DataFrame to store the extracted information\n",
    "df['Useful area'] = None\n",
    "df['Gross area'] = None\n",
    "df['Construction year'] = None\n",
    "df['Energetic certificate'] = None\n",
    "df['Enterprise'] = None\n",
    "df['Rooms'] = None\n",
    "df['Bathroom'] = None\n",
    "df['Description'] = None\n",
    "\n",
    "# Initialize UserAgent object\n",
    "ua = UserAgent()\n",
    "\n",
    "url_counter = 0\n",
    "\n",
    "for property_number, url in enumerate(df['URL']):\n",
    "    # Send a request to the URL with a random User-Agent\n",
    "    headers = {'User-Agent': ua.random}\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Get the useful_area from the property page\n",
    "        try:\n",
    "            # Extract useful_area if available (if not, set to 'N/A')\n",
    "            useful_area = \"N/A\"\n",
    "            df.at[property_number, 'Useful area'] = useful_area\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        # Get the gross_area, rooms, and bathrooms from the property page    \n",
    "        try:\n",
    "            # Find all <button> elements with the target class\n",
    "            buttons = soup.find_all('button', class_='eezlw8k1 css-ds0a69')\n",
    "            \n",
    "            # Ensure there are at least three buttons for area, rooms, and bathrooms\n",
    "            if len(buttons) >= 3:\n",
    "                # Extract area from the first button\n",
    "                area_div = buttons[0].find('div', class_='css-1ftqasz')\n",
    "                if area_div:\n",
    "                    area = area_div.get_text(strip=True)\n",
    "                    df.at[property_number, 'Gross area'] = area\n",
    "                else:\n",
    "                    df.at[property_number, 'Gross area'] = \"N/A\"\n",
    "                \n",
    "                # Extract rooms from the second button\n",
    "                rooms_div = buttons[1].find('div', class_='css-1ftqasz')\n",
    "                if rooms_div:\n",
    "                    rooms = rooms_div.get_text(strip=True)\n",
    "                    df.at[property_number, 'Rooms'] = rooms\n",
    "                else:\n",
    "                    df.at[property_number, 'Rooms'] = \"N/A\"\n",
    "                \n",
    "                # Extract bathrooms from the third button\n",
    "                bathrooms_div = buttons[2].find('div', class_='css-1ftqasz')\n",
    "                if bathrooms_div:\n",
    "                    bathrooms = bathrooms_div.get_text(strip=True)\n",
    "                    df.at[property_number, 'Bathroom'] = bathrooms\n",
    "                else:\n",
    "                    df.at[property_number, 'Bathroom'] = \"N/A\"\n",
    "            else:\n",
    "                # If fewer than three buttons are found, set all fields to \"N/A\"\n",
    "                df.at[property_number, 'Area'] = \"N/A\"\n",
    "                df.at[property_number, 'Rooms'] = \"N/A\"\n",
    "                df.at[property_number, 'Bathroom'] = \"N/A\"\n",
    "\n",
    "        except AttributeError:\n",
    "            # Handle potential errors gracefully\n",
    "            df.at[property_number, 'Area'] = \"N/A\"\n",
    "            df.at[property_number, 'Rooms'] = \"N/A\"\n",
    "            df.at[property_number, 'Bathrooms'] = \"N/A\"\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Get the construction year from the property page    \n",
    "        try:\n",
    "            # Find all div elements with the target class\n",
    "            divs = soup.find_all('div', class_='css-t7cajz e15n0fyo1')\n",
    "            \n",
    "            construction_year_found = False  # Flag to indicate if the construction year was found\n",
    "            \n",
    "            for div in divs:\n",
    "                # Find all <p> elements inside the current div\n",
    "                p_elements = div.find_all('p', class_='e15n0fyo2 css-nlohq6')\n",
    "                \n",
    "                # Iterate through the <p> elements to find the \"Ano de construção\"\n",
    "                for i, p in enumerate(p_elements):\n",
    "                    if \"Ano de construção\" in p.get_text(strip=True):\n",
    "                        # Ensure there is a next <p> element for the construction year\n",
    "                        if i + 1 < len(p_elements):\n",
    "                            construction_year = p_elements[i + 1].get_text(strip=True)\n",
    "                            df.at[property_number, 'Construction year'] = construction_year\n",
    "                            construction_year_found = True\n",
    "                            break\n",
    "                if construction_year_found:\n",
    "                    break\n",
    "            \n",
    "            # If no construction year was found in any of the divs\n",
    "            if not construction_year_found:\n",
    "                df.at[property_number, 'Construction year'] = \"N/A\"\n",
    "\n",
    "        except (AttributeError, IndexError):\n",
    "            # Handle potential errors gracefully\n",
    "            df.at[property_number, 'Construction year'] = \"N/A\"\n",
    "            pass\n",
    "\n",
    "        #Get the energetic certificate from the property page    \n",
    "        try:\n",
    "            # Find all div elements with the target class\n",
    "            divs = soup.find_all('div', class_='css-t7cajz e15n0fyo1')\n",
    "            \n",
    "            construction_year_found = False  # Flag to indicate if the construction year was found\n",
    "            \n",
    "            for div in divs:\n",
    "                # Find all <p> elements inside the current div\n",
    "                p_elements = div.find_all('p', class_='e15n0fyo2 css-nlohq6')\n",
    "                \n",
    "                # Iterate through the <p> elements to find the \"Certificado energético\"\n",
    "                for i, p in enumerate(p_elements):\n",
    "                    if \"Certificado energético\" in p.get_text(strip=True):\n",
    "                        # Ensure there is a next <p> element for the construction year\n",
    "                        if i + 1 < len(p_elements):\n",
    "                            construction_year = p_elements[i + 1].get_text(strip=True)\n",
    "                            df.at[property_number, 'Energetic certificate'] = construction_year\n",
    "                            construction_year_found = True\n",
    "                            break\n",
    "                if construction_year_found:\n",
    "                    break\n",
    "            \n",
    "            # If no construction year was found in any of the divs\n",
    "            if not construction_year_found:\n",
    "                df.at[property_number, 'Energetic certificate'] = \"N/A\"\n",
    "        except (AttributeError, IndexError):\n",
    "            # Handle potential errors gracefully\n",
    "            df.at[property_number, 'Energetic certificate'] = \"N/A\"\n",
    "            pass\n",
    "\n",
    "        try:\n",
    "            # Extract enterprise if available\n",
    "            enterprise = \"N/A\"\n",
    "            df.at[property_number, 'Enterprise'] = enterprise\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        \n",
    "        # Get the description from the property page\n",
    "        try:\n",
    "            # Extract useful_area if available (if not, set to 'N/A')\n",
    "            description = \"N/A\"\n",
    "            df.at[property_number, 'Description'] = description\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "\n",
    "        # Introduce a random delay between requests (e.g., between 0.5 and 1.5 seconds)\n",
    "        delay = random.uniform(0.5, 1.5)\n",
    "        time.sleep(delay)\n",
    "        # Increment the URL counter\n",
    "        url_counter += 1\n",
    "\n",
    "        # Check if 100 URLs have been processed and save the DataFrame\n",
    "        if url_counter % 100 == 0:\n",
    "            df.to_csv('Files/apartmentsPortugal.csv', index=False)\n",
    "            print(f\"Saved progress at {url_counter} URLs.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve property page for {url}\")\n",
    "\n",
    "# Save the updated DataFrame to the existing CSV file\n",
    "df.to_csv('Files/apartmentsPortugal.csv', index=False)\n",
    "\n",
    "print(\"Information extracted and added to apartmentsPortugal.csv.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "2oLu0hOuU5ba"
   },
   "outputs": [],
   "source": [
    "# Load your DataFrame containing property URLs\n",
    "df = pd.read_csv('Files/apartmentsPortugal.csv')\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "for property_number, url in enumerate(df['URL']):\n",
    "    # Send a request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the script tag containing image data\n",
    "        script_elements = soup.find_all('script')\n",
    "\n",
    "        for script_element in script_elements:\n",
    "            script_content = script_element.string\n",
    "            if script_content:\n",
    "                # Use regex to extract image URLs from the script\n",
    "                image_urls = re.findall(r'\"large\":\"(https://ireland\\.apollo\\.olxcdn\\.com[^\"]+)\"', script_content)\n",
    "                if image_urls:\n",
    "                    property_folder = f'Files/PropertiesImages/Property{property_number}'\n",
    "                    os.makedirs(property_folder, exist_ok=True)\n",
    "\n",
    "                    for index, image_url in enumerate(image_urls):\n",
    "                        image_response = requests.get(image_url)\n",
    "                        if image_response.status_code == 200:\n",
    "                            image_extension = \"webp\"  # Assuming images are in webp format\n",
    "                            image_filename = f\"Property{property_number}-Image{index + 1}.{image_extension}\"\n",
    "                            image_path = os.path.join(property_folder, image_filename)\n",
    "\n",
    "                            with open(image_path, 'wb') as f:\n",
    "                                f.write(image_response.content)\n",
    "                            print(f\"Image {index + 1} saved to {image_path}\")\n",
    "                        else:\n",
    "                            print(f\"Failed to download image {index + 1}\")\n",
    "                else:\n",
    "                    print(\"No image URLs found in script\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve property page for {url}\")\n",
    "\n",
    "print(\"Images downloaded and saved for all properties.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load your DataFrame containing property URLs\n",
    "df = pd.read_csv('Files/apartmentsPortugal.csv')\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Choose  a property to start from\n",
    "start_property = 13843  \n",
    "\n",
    "for property_number, url in enumerate(df['URL']):\n",
    "    if property_number < start_property:\n",
    "        continue  # Pula propriedades até atingir a propriedade inicial\n",
    "\n",
    "    # Send a request to the URL\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the script tag containing image data\n",
    "        script_elements = soup.find_all('script')\n",
    "\n",
    "        for script_element in script_elements:\n",
    "            script_content = script_element.string\n",
    "            if script_content:\n",
    "                # Use regex to extract image URLs from the script\n",
    "                image_urls = re.findall(r'\"large\":\"(https://ireland\\.apollo\\.olxcdn\\.com[^\"]+)\"', script_content)\n",
    "                if image_urls:\n",
    "                    property_folder = f'Files/PropertiesImages/Property{property_number}'\n",
    "                    os.makedirs(property_folder, exist_ok=True)\n",
    "\n",
    "                    for index, image_url in enumerate(image_urls):\n",
    "                        image_response = requests.get(image_url)\n",
    "                        if image_response.status_code == 200:\n",
    "                            image_extension = \"webp\"  # Assuming images are in webp format\n",
    "                            image_filename = f\"Property{property_number}-Image{index + 1}.{image_extension}\"\n",
    "                            image_path = os.path.join(property_folder, image_filename)\n",
    "\n",
    "                            with open(image_path, 'wb') as f:\n",
    "                                f.write(image_response.content)\n",
    "                            print(f\"Image {index + 1} saved to {image_path}\")\n",
    "                        else:\n",
    "                            print(f\"Failed to download image {index + 1}\")\n",
    "                else:\n",
    "                    print(\"No image URLs found in script\")\n",
    "    else:\n",
    "        print(f\"Failed to retrieve property page for {url}\")\n",
    "\n",
    "print(\"Images downloaded and saved for all properties.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
